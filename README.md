# DeepSeek-from-Scratch
This repository contains an ongoing implementation of DeepSeek from scratch. Currently, it focuses on building core components such as Multi-Head Latent Attention (MLA) â€” a memory-efficient alternative to standard Multi-Head Attention, used in models like DeepSeek-V2/V3. 
